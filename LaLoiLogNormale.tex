%% LyX 2.2.1 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[french,12pt]{paper}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[landscape,a4paper]{geometry}
\geometry{verbose,tmargin=19mm,bmargin=0mm,lmargin=16mm,rmargin=16mm}
\pagestyle{empty}
\setlength{\parskip}{\medskipamount}
\setlength{\parindent}{0pt}
\usepackage{fancybox}
\usepackage{calc}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{esint}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{shapes}
\usepackage{mathrsfs}
\usepackage{mathabx}
\usepackage{txfonts}
\usepackage{pxfonts}
\usepackage{titling}
\usepackage{array}
\usepackage{bbold}
%\usepackage{yhmath}

\newdimen\un \un=.5mm
\def\bordure{
\begin{tikzpicture}[overlay,remember picture]
\def\p{.75}
\def\ang{45}
\def\alp{160.2}
\def\bet{72.42}
\def\gam{-13.2}
\draw [very thick, line width = 8pt, color = red!25!blue!33.333!green!50]
($(current page.north west)+(20*\un,-20*\un)$)
-- ($(current page.north east)+(-24*\un,-20*\un)$)
node [draw, ellipse, fill, text=white, pos=.53] {\thetitle}
arc (90 : 0 : 4*\un)
-- ($(current page.south east)+(-20*\un,24*\un)$)
node[text=white,pos=.54 , rotate=90]
{\tiny Ce document est sous licence GNU FDL,
 il est librement modifiable et distribuable.
 Licence et sources complètes disponibles sur le site.
 Copyright 2016, Jean-Christophe Jameux}
arc (0 : -90 : 4*\un)
-- ($(current page.south west)+(24*\un,20*\un)$)
node[draw, ellipse, fill, text=white, pos=.3] {\bf\large Echologie.org}
arc (-90 : -180 : 4*\un)
-- ($(current page.north west)+(20*\un,-20*\un)$);

\un=.9mm
\node(Triskell) at ($(current page.north west)+(15*\un,-11*\un)$){};
\draw [fill = white, color = red!25!blue!33.333!green!50]
(Triskell) + (1.2*\un,-6*\un) circle (15*\un);
\draw [fill = white, color = white] (Triskell) circle (2*\un);
\draw [fill = white, color = white]
(Triskell) + ({120*(1+\p)} : 3*\un)
arc ({120*(1+\p)} : 120 : 3*\un)
arc (180+\ang : 180-\ang :3*\un)
arc (\alp-\ang : \alp+\ang+24.8 : 5*\un);
\draw [fill = white, color = white]
(Triskell) + (120*\p : 3*\un)
arc (120*\p : 0 : 3*\un)
arc (90+\ang : 90-\ang : 6*\un)
arc (\bet-\ang : \bet+\ang+5.9 : 8*\un);
\draw [fill = white, color = white]
(Triskell) + ({120*(2+\p)} : 3*\un)
arc ({120*(2+\p)} : 240 : 3*\un)
arc (\ang : -\ang : 12*\un)
arc (\gam-\ang : \gam+\ang+.85 : 13*\un);
\end{tikzpicture}}

\makeatother

\usepackage{babel}
\makeatletter
\addto\extrasfrench{%
   \providecommand{\og}{\leavevmode\flqq~}%
   \providecommand{\fg}{\ifdim\lastskip>\z@\unskip\fi~\frqq}%
}

\makeatother
\begin{document}
\title{\large\bf La loi log-normale}

A\hphantom{Deca}Connaître une variable aléatoire, c'est avant tout
connaître sa loi. Dans le cas d'une variable aléatoire réelle à densité
$X$, cette loi est \hphantom{Dec.}entièrement déterminée aussi bien
par sa fonction de répartition $F_{X}:x\mapsto P\left(X\leq x\right)$
que par sa fonction de densité $f_{X}\coloneqq F_{X}'$. Chacune de
\hphantom{D}ces représentations de la loi de X se montre plus ou
moins adaptée à la résolution de certains problèmes. On est donc souvent
amené à voyager d'une représentation à l'autre. L'étude de la loi
log-normale sera l'occasion d'illustrer différentes dynamiques qui
se jouent entre fonctions de répartitions et fonctions de densités
dans l'étude des lois de variables aléatoires. Elle sera aussi l'occasion
de montrer à quel point il est important de connaître intimement les
lois de variables aléatoires usuelles.

\begin{minipage}[t]{0.48\columnwidth}%
On dit qu'une variable aléatoire $X$ suit une loi log-normale de
paramètre $\left(\mu,\sigma^{2}\right)$, ce qu'on note $\mathrm{X\sim Log}\!-\mathcal{N}(\mu,\,\sigma^{2})$,
si son logarithme suit une loi normale de paramètre $\left(\mu,\sigma^{2}\right)$.
Autrement dit, si elle est de la forme $X=e^{T}$ où $T\sim\mathcal{N}(\mu,\sigma^{2})$
; son logarithme est alors bien normal, autrement dit $\ln X\sim\mathcal{N}(\mu,\sigma^{2})$.
Nous allons montrer comment étudier une telle loi en nous ramenant
à des lois normales. 

Pour cela, notons $\Phi$ la fonction de répartition de la loi normale
centrée réduite et $\varphi$ sa fonction de densité.

Comme on connait la relation entre $X$ et $T$, il est aisé d'établir
une relation entre leurs fonctions de répartitions, et donc de déterminer
la fonction de répartition de $X$ :

\begin{minipage}[t]{0.4\columnwidth}%
\begin{flushleft}
\vspace{-3ex}
\begin{align*}
F_{X}\left(x\right):x & \mapsto P\left(X\leq x\right)\\
 & \mapsto P\left(e^{T}\leq x\right)\\
 & \mapsto\begin{cases}
P\left(T\leq\ln x\right) & \mathnormal{si}\;x>0\\
0 & \mathnormal{si}\;x\leq0
\end{cases}\\
 & \mapsto P\left(T\leq\ln x\right)\cdot\mathbb{1}_{\left]0,+\infty\right[}\left(x\right)\\
 & \mapsto F_{T}(\ln x)\cdot\mathbb{1}_{\left]0,+\infty\right[}\left(x\right)\\
 & \mapsto\Phi\left(\frac{\ln x-\mu}{\sigma}\right)\cdot\mathbb{1}_{\left]0,+\infty\right[}\left(x\right)
\end{align*}
\par\end{flushleft}%
\end{minipage}\hfill{}%
\begin{minipage}[t]{0.44\columnwidth}%
La fonction indicatrice d'un ensemble $E$, notée $\mathbb{1}_{E}$
est définie par :\vspace{-3ex}
\[
\forall x\in\mathbb{R},\mathbb{1}_{E}=\begin{cases}
1 & \textrm{si }x\in E\\
0 & \textrm{si }x\notin E
\end{cases}
\]

\vspace{-1ex}

Elle permet une écriture plus compacte, et donc plus commode, des
fonctions ayant des formules algébriques différentes selon les parties
de $\mathbb{R}$ où elles sont définies.%
\end{minipage}

\vspace{1ex}
Un calcul direct nous permet alors de trouver une expression de la
densité de $X$:

\medskip{}

\hspace{6em}%
{\fboxsep 1.5ex\Ovalbox{\begin{minipage}[t]{0.68\columnwidth}%
${\displaystyle f_{X}:x\mapsto F_{X}'\left(x\right)=\frac{1}{\sigma x}\cdot\varphi\left(\frac{\ln x-\mu}{\sigma}\right)\cdot\mathbb{1}_{\left]0,+\infty\right[}\left(x\right)}$%
\end{minipage}}}%
\end{minipage}\hfill{}%
\begin{minipage}[t]{0.5\columnwidth}%
\begin{minipage}[t]{0.52\columnwidth}%
Si la relation entre $F_{T}$ et $\Phi$ ne vous apparaît pas clairement,
voyons comment la retrouver en trouvant une relation entre $f_{T}$
et $\varphi$. Le calcul ci-contre vise à \emph{reconnaitre} une loi
normale centrée réduite dans l'\emph{écriture} d'une loi normale quelconque.
Mais nous ne savons \emph{reconnaitre} une telle loi que sur sa densité,
c'est pourquoi nous écrivons la fonction de répartition comme une
intégrale de la fonction de densité. On utilisera pour ce faire le
changement de variable :\vspace{-1ex}
\[
\tau=\frac{t-\mu}{\sigma}\textrm{ \ (d'où }dt=\sigma d\tau\textrm{)}
\]
%
\end{minipage} \hfill{}%
\begin{minipage}[t]{0.45\columnwidth}%
\vspace{-7ex}
\begin{align*}
F_{T}:u & \mapsto\intop_{-\infty}^{u}f_{T}\left(t\right)dt\\
 & \mapsto\intop_{-\infty}^{u}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(t-\mu)^{2}}{2\sigma^{2}}}dt\\
 & \mapsto\intop_{-\infty}^{\frac{u-\mu}{\sigma}}\frac{1}{\sqrt{2\pi}}e^{-\frac{\tau^{2}}{2}}d\tau\\
 & \mapsto\intop_{-\infty}^{\frac{u-\mu}{\sigma}}\varphi(\tau)d\tau\\
 & \mapsto\Phi\left(\frac{u-\mu}{\sigma}\right)
\end{align*}
%
\end{minipage}

\vspace{1ex}

De même, s'il ne vous a pas échappé qu'on a pas pris garde à traiter
correctement le cas de la dérivée de $F_{X}$ en 0, voici un moyen
direct de démontrer qu'on a bien $F_{X}'(0)=0$. Comme $\lim_{x\to0^{+}}\frac{\ln x-\mu}{\sigma}=-\infty$,
on peut affirmer l'existence d'un $\alpha$ pour lequel $\forall x\in]0;\alpha[,\forall\tau<\frac{\ln x-\mu}{\sigma},-\tau>\frac{1}{\sqrt{2\pi}}$.
Dés lors, le calcul suivant achève la démonstration :{\footnotesize{}\vspace{-1ex}
\begin{align*}
 & \forall x\in\left]0;\alpha\right[,\frac{F_{X}(x)-F_{X}(0)}{x}=\frac{1}{x}\cdot\Phi\left(\frac{\ln x-\mu}{\sigma}\right)=\frac{1}{x}\cdot\intop_{-\infty}^{\frac{\ln x-\mu}{\sigma}}\frac{1}{\sqrt{2\pi}}e^{-\frac{\tau^{2}}{2}}d\tau\\
 & <\frac{1}{x}\cdot\intop_{-\infty}^{\frac{\ln x-\mu}{\sigma}}-\tau.e^{-\frac{\tau^{2}}{2}}d\tau=\frac{1}{x}\cdot e^{-\frac{\left(\ln x-\mu\right)^{2}}{2\sigma^{2}}}=e^{-\ln x}\cdot e^{-\frac{\left(\ln x-\mu\right)^{2}}{2\sigma^{2}}}=e^{-\frac{\left(\ln x\right)^{2}+2\left(\sigma^{2}-\mu\right)\ln x+\mu^{2}}{2\sigma^{2}}}\xrightarrow[x\to0^{+}]{}0
\end{align*}
}{\footnotesize \par}%
\end{minipage} 

\bordure

\newpage{}

\title{\large\bf Moments d'une loi log-normale}

A\hphantom{Deca} L'inégalité de Markov prétend que pour toute variable
aléatoire positive et pour tout $a>0$, on a $P\left(A\geq a\right)\leq\frac{1}{a}\cdot E\left(A\right)$.\linebreak{}
 \hphantom{Deca}En prenant $A$ de la forme $\left|X-E\left(X\right)\right|^{n}$
avec $n\in\mathbb{N}$ et en posant $s\coloneqq\sqrt[n]{a}$, on obtient
l'inégalité $P\left(\left|X-E\left(X\right)\right|\geq s\right)\leq\frac{1}{s^{n}}\cdot E\left(\left|X-E\left(X\right)\right|^{n}\right)$.
\hphantom{D}En particulier, dans le cas où $n=2$, en posant $k\coloneqq\frac{s}{\sigma}$,
on obtient l'inégalité de Bienaymé-Tchebychev $P\left(\left|X-E\left(X\right)\right|\geq k\sigma\right)\leq\frac{1}{\left(k\sigma\right)^{2}}\cdot V(X)=\frac{1}{k^{2}}$.\linebreak{}
De telles inégalités permettent de majorer de façon probable l'écart
d'une variable aléatoire à sa valeur moyenne. Elle sont à ce titre
fondamentales pour réaliser des estimations, par exemple en statistiques.
C'est la raison pour laquelle, outre la valeur de $E\left(X\right)$,
on est intéressé plus générallement par la valeur de $E\left(X^{n}\right)$
pour tout $n\in\mathbb{N}^{*}$, cette quantité est appelée le moment
d'ordre $n$ de $X$.

\begin{minipage}[t]{0.43\columnwidth}%
\vspace{-2em}
\begin{minipage}[t]{0.55\columnwidth}%
\vspace{3em}
En remarquant que :
\[
f_{X}\equiv0\textrm{ sur }\left]-\infty;0\right]
\]
et en effectuant le changement de variable : 
\[
t=\frac{\ln x-\mu}{\sigma}\textrm{ \ (d'où }dx=\sigma e^{\sigma t+\mu}dt\textrm{)}
\]

\vspace{-1ex}

nous sommes maintenant à même de calculer l'espérance de $X$. La
clef de notre calcul tient dans la remarque suivante : la fonction
définie sur $\mathbb{R}$ par :
\[
t\mapsto\frac{1}{\sqrt{2\pi}}e^{-\frac{(t-\sigma)^{2}}{2}}
\]

\vspace{-1ex}

n'est autre que la fonction de densité d'une loi normale $\mathcal{N}(\sigma,1)$,
d'où :

\vspace{-1ex}
\[
\intop_{-\infty}^{+\infty}\frac{1}{\sqrt{2\pi}}e^{-\frac{(t-\sigma)^{2}}{2}}dt=1
\]
%
\end{minipage}\hspace{-3ex}%
\begin{minipage}[t]{0.4\columnwidth}%
\begin{align*}
E\left(X\right) & =\intop_{-\infty}^{+\infty}x\cdot f_{X}\left(x\right)dx\\
 & =\intop_{0}^{+\infty}\frac{1}{\sigma}\cdot\varphi\left(\frac{\ln x-\mu}{\sigma}\right)dx\\
 & =\intop_{-\infty}^{+\infty}\varphi\left(t\right)e^{\sigma t+\mu}dt\\
 & =\intop_{-\infty}^{+\infty}\frac{1}{\sqrt{2\pi}}e^{-\frac{t^{2}}{2}}e^{\sigma t+\mu}dt\\
 & =\intop_{-\infty}^{+\infty}\frac{1}{\sqrt{2\pi}}e^{-\frac{t^{2}-2\sigma t-2\mu}{2}}dt\\
 & =\intop_{-\infty}^{+\infty}\frac{1}{\sqrt{2\pi}}e^{-\frac{\left(t-\sigma\right)^{2}-\sigma^{2}-2\mu}{2}}dt\\
 & =e^{\frac{\sigma^{2}+2\mu}{2}}\intop_{-\infty}^{+\infty}\frac{1}{\sqrt{2\pi}}e^{-\frac{\left(t-\sigma\right)^{2}}{2}}dt\\
 & =e^{\mu+\frac{\sigma^{2}}{2}}
\end{align*}
%
\end{minipage}%
\end{minipage}\hfill{}%
\begin{minipage}[t]{0.54\columnwidth}%
Un moyen direct de déterminer les moments de $X$ serait de faire
appel au lemme de transfert, en calculant $\int_{-\infty}^{+\infty}x^{n}\cdot f_{X}\left(x\right)dx$
pour chaque $n\in\mathbb{N}^{*}$. Nous allons nous montrer plus futés
! Nous allons montrer que la variable aléatoire $Y$ définie par $Y=X^{n}$
suit une loi log-normale dont on va chercher les paramètres. On pourra
alors appliquer la formule de l'espérance que nous venons de démontrer
en utilisant les paramètres ainsi trouvés.

Pour démontrer que $Y$ suit une loi log-normale, il suffit de la
mettre sous la forme $e^{T'}$où $T'$ suit une loi normale.\vspace{-1.5ex}
\[
Y=X^{n}=\left(e^{T}\right)^{n}=e^{nT}
\]

\vspace{-1.5ex}

Il s'agit donc de montrer que $T':=nT$ suit une loi normale et d'en
trouver les paramètres.

\vspace{-4.5ex}

\begin{minipage}[c]{0.4\columnwidth}%
\begin{align*}
F_{T'}:t & \mapsto P\left(T'\leq t\right)\\
 & \mapsto P\left(nT\leq t\right)\\
 & \mapsto P\left(T\leq\frac{t}{n}\right)\\
 & \mapsto F_{T}\left(\frac{t}{n}\right)
\end{align*}
%
\end{minipage}D'où%
\begin{minipage}[c][1\totalheight][t]{0.45\columnwidth}%
\begin{align*}
f_{T'}:t & \mapsto F_{T'}'\left(t\right)\\
 & \mapsto\frac{1}{n}f_{T}\left(\frac{t}{n}\right)\\
 & \mapsto\frac{1}{\sqrt{2\pi}n\sigma}e^{-\frac{\left(\frac{t}{n}-\mu\right)^{2}}{2\sigma^{2}}}\\
 & \mapsto\frac{1}{\sqrt{2\pi}n\sigma}e^{-\frac{\left(t-n\mu\right)^{2}}{2\left(n\sigma\right)^{2}}}
\end{align*}
%
\end{minipage}

\vspace{0ex}

Nous avons ainsi $T'\sim\mathcal{N}(n\mu,\left(n\sigma\right)^{2})$.
Pour arriver à ce résultat, nous avons manipulé la fonction de densité
de $T'$ dans le but de \textit{faire apparaître}\emph{ }la densité
d'une loi normale. C'est d'ailleurs pour cette raison que nous sommes
passé par la densité, comme quand nous avons cherché une relation
entre $F_{T}$ et $\Phi$, c'est le moyen le plus simple de reconnaître
une loi normale.

Nous pouvons maintenant conclure fièrement :

\vspace{-1ex}

\hspace{22em}%
{\fboxsep 1.5ex\Ovalbox{\begin{minipage}[t]{0.22\columnwidth}%
\hfill{}${\displaystyle E\left(X^{n}\right)=e^{n\mu+\frac{n^{2}}{2}\sigma^{2}}}$\hfill{}%
\end{minipage}}}%
\end{minipage}

\bordure
\end{document}
